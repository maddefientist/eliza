"use strict";(self.webpackChunk_elizaos_docs=self.webpackChunk_elizaos_docs||[]).push([[32285],{14160:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"namespaces/v2/interfaces/TokenizeTextParams","title":"TokenizeTextParams","description":"@elizaos/core v1.0.2 / v2 / TokenizeTextParams","source":"@site/api/namespaces/v2/interfaces/TokenizeTextParams.md","sourceDirName":"namespaces/v2/interfaces","slug":"/namespaces/v2/interfaces/TokenizeTextParams","permalink":"/api/namespaces/v2/interfaces/TokenizeTextParams","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"TextToSpeechParams","permalink":"/api/namespaces/v2/interfaces/TextToSpeechParams"},"next":{"title":"TranscriptionParams","permalink":"/api/namespaces/v2/interfaces/TranscriptionParams"}}');var i=s(31085),r=s(71184);const a={},o="Interface: TokenizeTextParams",c={},d=[{value:"Properties",id:"properties",level:2},{value:"prompt",id:"prompt",level:3},{value:"Defined in",id:"defined-in",level:4},{value:"modelType",id:"modeltype",level:3},{value:"Defined in",id:"defined-in-1",level:4}];function l(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/api/",children:"@elizaos/core v1.0.2"})," / ",(0,i.jsx)(n.a,{href:"/api/namespaces/v2/",children:"v2"})," / TokenizeTextParams"]}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"interface-tokenizetextparams",children:"Interface: TokenizeTextParams"})}),"\n",(0,i.jsxs)(n.p,{children:["Parameters for tokenizing text, i.e., converting a string into a sequence of numerical tokens.\nThis is a common preprocessing step for many language models.\nThis structure is used with ",(0,i.jsx)(n.code,{children:"AgentRuntime.useModel"})," when the ",(0,i.jsx)(n.code,{children:"modelType"})," is ",(0,i.jsx)(n.code,{children:"ModelType.TEXT_TOKENIZER_ENCODE"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"properties",children:"Properties"}),"\n",(0,i.jsx)(n.h3,{id:"prompt",children:"prompt"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"prompt"}),": ",(0,i.jsx)(n.code,{children:"string"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The input string to be tokenized."}),"\n",(0,i.jsx)(n.h4,{id:"defined-in",children:"Defined in"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://github.com/maddefientist/eliza/blob/main/packages/core/src/specs/v2/types.ts#L1401",children:"packages/core/src/specs/v2/types.ts:1401"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"modeltype",children:"modelType"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"modelType"}),": ",(0,i.jsx)(n.code,{children:"string"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The model type to use for tokenization, which determines the tokenizer algorithm and vocabulary."}),"\n",(0,i.jsx)(n.h4,{id:"defined-in-1",children:"Defined in"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://github.com/maddefientist/eliza/blob/main/packages/core/src/specs/v2/types.ts#L1403",children:"packages/core/src/specs/v2/types.ts:1403"})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},71184:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var t=s(14041);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);